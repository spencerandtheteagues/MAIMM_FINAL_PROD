here’s the exact, production-ready way to wire up text (Gemini 2.5 Pro), images (Imagen 4), and video (Veo 3) in a web app using Google’s unified Gen AI SDK. This uses the same code path whether you run against Vertex AI (recommended for prod) or the Gemini Developer API (quick prototyping). Model IDs and limits are straight from the current docs; I’ve cited them so you know nothing here is guesswork. 
Google Cloud
+3
Google Cloud
+3
Google Cloud
+3

0) Model IDs you’ll actually use

Text: gemini-2.5-pro (GA on Vertex). 
Google Cloud

Image: imagen-4.0-generate-001 (high-fidelity image generation). 
Google Cloud

Video (quality): veo-3.0-generate-001 (8-second video; 720p/1080p; audio on). 
Google Cloud

Video (faster/cheaper): veo-3.0-fast-generate-001. 
Google Cloud

Notes you’ll want to remember:

Imagen 4 supports multiple aspect ratios and returns up to 4 images per request; images are SynthID watermarked. 
Google Cloud

Veo 3 outputs 8-second clips and watermarks with SynthID; person-generation settings vary by region. 
Google Cloud
Google AI for Developers

1) Install + auth (same SDK for Vertex or Gemini API)
# JS/TS server
npm i @google/genai

# (Vertex) set ADC + project/region
# Use 'global' unless you must pin to a region available to the model.
setx GOOGLE_GENAI_USE_VERTEXAI true
setx GOOGLE_CLOUD_PROJECT your-gcp-project-id
setx GOOGLE_CLOUD_LOCATION global

# OR (Gemini API) use an API key
setx GOOGLE_API_KEY your_ai_studio_key   # pass via code as apiKey


The Gen AI SDK lets you point the same code at Vertex (ADC creds) or Gemini API (key). 
Google Cloud

2) Backend: Express routes you can drop in

Create aiRoutes.ts (or .js) with three endpoints: text, image, video (start + poll). Keep your keys server-side; never expose them to the browser.

/**
 * aiRoutes.ts – Express routes for Gemini 2.5 Pro, Imagen 4, Veo 3
 */
import express from "express";
import { GoogleGenAI } from "@google/genai";

const router = express.Router();

// Toggle Vertex vs Gemini API from env; both work with the same SDK.
const ai = new GoogleGenAI({
  vertexai: !!process.env.GOOGLE_GENAI_USE_VERTEXAI,
  // For Gemini API path only; Vertex uses ADC by default.
  apiKey: process.env.GOOGLE_API_KEY || process.env.GEMINI_API_KEY,
  project: process.env.GOOGLE_CLOUD_PROJECT,
  location: process.env.GOOGLE_CLOUD_LOCATION || "global",
});

/**
 * 2.1 TEXT (Gemini 2.5 Pro)
 */
router.post("/text", async (req, res) => {
  try {
    const { prompt, system, jsonSchema, temperature = 0.9, maxOutputTokens = 2048 } = req.body;

    const response = await ai.models.generateContent({
      model: "gemini-2.5-pro",
      contents: [
        ...(system ? [{ role: "system", parts: [{ text: system }] }] : []),
        { role: "user", parts: [{ text: prompt }] },
      ],
      generationConfig: {
        temperature,
        maxOutputTokens,
        responseMimeType: jsonSchema ? "application/json" : undefined,
        responseSchema: jsonSchema || undefined, // Enforces structured JSON if provided
      },
      // safetySettings: [...] // optional: configure safety if your app requires stricter filters
    });

    res.json({ text: response.text() });
  } catch (err: any) {
    console.error("text error", err);
    res.status(500).json({ error: err.message || "text_generation_failed" });
  }
});

/**
 * 2.2 IMAGE (Imagen 4)
 * Returns base64 data URLs so the browser can drop them straight into <img>.
 */
router.post("/image", async (req, res) => {
  try {
    const {
      prompt,
      aspectRatio = "1:1", // 1:1, 3:4, 4:3, 9:16, 16:9 (Imagen 4 supports these)
      personGeneration = "allow_all", // obey your regional policy & product rules
      count = 1, // up to 4 per request
      negativePrompt,
      outputResolution = "1024", // or 2048, 1408x768 etc. (see docs)
    } = req.body;

    const response = await ai.models.generateImages({
      model: "imagen-4.0-generate-001",
      prompt,
      config: {
        aspectRatio,
        personGeneration,          // region restrictions may apply for “person” content
        negativePrompt,            // omit unwanted content
        outputMimeType: "image/png",
        // Some SDKs name this "size" or "outputDimensions"; GenAI handles common forms.
        // For higher res, use preset aspect ratio + supported resolutions.
      },
      // images=...  // alternatively seed/init image for variations if you add that path later
      n: Math.min(Math.max(count, 1), 4),
    });

    const images = (response.generatedImages || []).map((img: any) => {
      const b64 = img.image.imageBytes?.toString("base64") || Buffer.from(img.image).toString("base64");
      return `data:image/png;base64,${b64}`;
    });

    res.json({ images, watermark: "SynthID (applied by model)" });
  } catch (err: any) {
    console.error("image error", err);
    res.status(500).json({ error: err.message || "image_generation_failed" });
  }
});

/**
 * 2.3 VIDEO (Veo 3) – long-running job: start + poll
 * Use 'veo-3.0-fast-generate-001' for faster/cheaper runs, 'veo-3.0-generate-001' for highest quality.
 */
router.post("/video/start", async (req, res) => {
  try {
    const {
      prompt,
      negativePrompt,
      aspectRatio = "16:9", // Veo 3 supports 16:9 and 9:16 on Vertex
      fast = true,
      // If you want image-to-video later, accept an 'imageDataUrl' and pass as { imageBytes, mimeType }
    } = req.body;

    const model = fast ? "veo-3.0-fast-generate-001" : "veo-3.0-generate-001";

    const op = await ai.models.generateVideos({
      model,
      prompt,
      config: {
        aspectRatio,
        negativePrompt,
        // personGeneration: "allow_all" | "allow_adult" | "dont_allow"
        //     (check regional policy; see docs)
      },
      // image: { imageBytes, mimeType }  // for image-to-video
    });

    // Return the operation name so the client can poll
    res.json({ operationName: op.name || op.operation || op });
  } catch (err: any) {
    console.error("video start error", err);
    res.status(500).json({ error: err.message || "video_start_failed" });
  }
});

router.get("/video/status/:operationName", async (req, res) => {
  try {
    const { operationName } = req.params;
    let op = await ai.operations.getVideosOperation({ operation: { name: operationName } as any });

    if (!op.done) return res.json({ done: false });

    // When done, grab the first video and stream URL or bytes
    const video = op.response?.generatedVideos?.[0];
    if (!video) return res.status(500).json({ error: "no_video_in_response" });

    // You can either download server-side and host it, or hand the signed URI to the client:
    // 1) Hand back a signed download URI (simplest):
    if (video.video?.uri) {
      return res.json({ done: true, uri: video.video.uri, hasAudio: true, lengthSeconds: 8 });
    }

    // 2) Or download bytes to server, then serve from your storage:
    // const file = await ai.files.download({ file: video.video });
    // const mp4 = Buffer.from(file.data ?? file); // depends on SDK version
    // ...upload to Cloud Storage/S3...
    // return res.json({ done: true, uri: publicUrl });

    return res.json({ done: true, note: "Video completed but no URI; use files.download to fetch bytes." });
  } catch (err: any) {
    console.error("video poll error", err);
    res.status(500).json({ error: err.message || "video_poll_failed" });
  }
});

export default router;


Why this shape?

Text is synchronous.

Images are synchronous and return up to 4 results; you return data URLs so the browser can render immediately. 
Google Cloud

Video is an LRO (long-running operation). You start it, get an operationName, and poll until done: true, then either return a signed download URI (Gemini API path) or download bytes (Vertex path), both supported by the SDK. 
Google AI for Developers

Wire it into your app:

// server.ts
import express from "express";
import cors from "cors";
import aiRoutes from "./aiRoutes";

const app = express();
app.use(cors());
app.use(express.json({ limit: "10mb" })); // allow image data URLs if you add img2vid
app.use("/api/ai", aiRoutes);

app.listen(process.env.PORT || 8080, () =>
  console.log("AI routes ready on /api/ai")
);

3) Frontend usage (React/Vite example)
// Text
const r = await fetch("/api/ai/text", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({
    system: "You write punchy, on-brand social captions.",
    prompt: "14 short posts for a Nashville coffee truck; include hashtags.",
    jsonSchema: null, // or provide a JSON schema if you want structured output
  }),
});
const { text } = await r.json();

// Image
const ri = await fetch("/api/ai/image", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({
    prompt:
      "Photorealistic latte in golden-hour light on a reclaimed wood counter; handwritten 'Nash Roast' cup.",
    aspectRatio: "4:3",
    count: 2,
    negativePrompt: "blurry, watermark, low quality",
  }),
});
const { images } = await ri.json(); // array of data URLs

// Video (start + poll)
const rv = await fetch("/api/ai/video/start", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({
    prompt:
      "Cinematic slow pan across a latte art heart, gentle steam, soft jazz; barista says 'Fuel for your hustle.'",
    aspectRatio: "16:9",
    fast: true, // set false to use the quality model
  }),
});
const { operationName } = await rv.json();

let uri = "";
while (true) {
  const s = await fetch(`/api/ai/video/status/${operationName}`).then((x) => x.json());
  if (s.done) { uri = s.uri; break; }
  await new Promise((r) => setTimeout(r, 5000));
}
// <video controls src={uri} />

4) Parameters that matter (and the gotchas)

Gemini 2.5 Pro: pass generationConfig for temperature, max tokens; add responseSchema to force strict JSON for programmatic use (fortifies your credit/automation flows). Model ID: gemini-2.5-pro. 
Google Cloud

Imagen 4: use aspectRatio (1:1, 3:4, 4:3, 9:16, 16:9) and request up to 4 images. The SDK applies SynthID watermark automatically. For product safety/scoped use, toggle personGeneration and negativePrompt to keep outputs brand-safe. Model ID: imagen-4.0-generate-001. 
Google Cloud

Veo 3: it’s 8 seconds by design; choose quality (veo-3.0-generate-001) or fast (veo-3.0-fast-generate-001). Handle as an async operation; poll operations.getVideosOperation until done. Audio is included; assets are SynthID-watermarked. Regional limits apply for personGeneration; follow docs. 
Google Cloud
+1
Google AI for Developers

Switching backends: the Gen AI SDK supports Vertex (ADC, project/location) and Gemini API (key) with the same methods (generateContent, generateImages, generateVideos, operations.get…). This is deliberate so you can prototype on keys then move to Vertex without a rewrite. 
Google Cloud

5) Production knobs (performance, cost, safety)

Pick the cheapest that still looks good:

Text: use Pro for planning/campaign logic; you can fall back to 2.5 Flash for bulk variants. 
Google Cloud

Images: start with imagen-4.0-generate-001; if cost spikes, trial imagen-4.0-fast-generate-001. 
Google Cloud

Video: default to Veo 3 Fast for A/B ad creatives; switch to quality for the winners. 
Google Cloud

Time out your polls: request latency for Veo can range from ~seconds to minutes; use exponential backoff and a job table so you don’t pin browser tabs. 
Google AI for Developers

Respect file and token limits: Imagen max 4 images / request; Imagen prompt text is limited (≈480 tokens); Gemini/Pro has generous multimodal limits; keep request bodies small over HTTP. 
Google Cloud
+1

Watermarking is on: Images/videos carry SynthID; that’s a feature, not a bug. If your brand requires visible marks, add them post-hoc. 
Google Cloud
Google AI for Developers

Privacy & safety: lock “person generation” to your allowed locales in backend, not frontend. Veo has region-specific constraints; enforce server-side. 
Google AI for Developers

6) Minimal Python (FastAPI) version (if your backend is Python)
# pip install google-genai fastapi uvicorn
from fastapi import FastAPI
from google import genai
from google.genai import types

app = FastAPI()
client = genai.Client(  # Vertex path if env GOOGLE_GENAI_USE_VERTEXAI=true
    api_key=None  # Use if calling Gemini API directly: api_key=os.getenv("GOOGLE_API_KEY")
)

@app.post("/api/ai/text")
async def text(body: dict):
    r = client.models.generate_content(
        model="gemini-2.5-pro",
        contents=[{"role": "user", "parts": [{"text": body["prompt"]}]}],
        generation_config=types.GenerationConfig(temperature=0.9, max_output_tokens=2048),
    )
    return {"text": r.text()}

@app.post("/api/ai/image")
async def image(body: dict):
    r = client.models.generate_images(
        model="imagen-4.0-generate-001",
        prompt=body["prompt"],
        config=types.GenerateImagesConfig(aspect_ratio=body.get("aspectRatio","1:1"),
                                          output_mime_type="image/png"),
        n=min(int(body.get("count",1)),4)
    )
    imgs = []
    for g in r.generated_images:
        b64 = g.image.image_bytes.encode_base64().decode() if hasattr(g.image,"image_bytes") else g.image
        imgs.append(f"data:image/png;base64,{b64}")
    return {"images": imgs}

@app.post("/api/ai/video/start")
async def video_start(body: dict):
    model = "veo-3.0-fast-generate-001" if body.get("fast", True) else "veo-3.0-generate-001"
    op = client.models.generate_videos(
        model=model,
        prompt=body["prompt"],
        config=types.GenerateVideosConfig(aspect_ratio=body.get("aspectRatio","16:9"))
    )
    return {"operationName": op.name}

@app.get("/api/ai/video/status/{name}")
async def video_status(name: str):
    op = client.operations.get(types.GenerateVideosOperation(name=name))
    if not op.done:
        return {"done": False}
    vid = op.response.generated_videos[0]
    if getattr(vid.video, "uri", None):
        return {"done": True, "uri": vid.video.uri}
    return {"done": True, "note": "Use files.download to fetch bytes"}


Same model IDs, same behavior; it’s just Python instead of Node. 
Google Cloud
+3
Google Cloud
+3
Google Cloud
+3

7) Common failure modes (so you don’t redeploy 50 times)

Auth path mismatch (using API key while vertexai: true, or ADC with vertexai: false). Align backend config with your chosen backend. The SDK intentionally supports both; don’t mix them. 
Google Cloud

Forgetting LRO polling for Veo 3. Start job ➜ poll ➜ download. If you try to “await video” like a text call, you’ll stall or error. 
Google AI for Developers

Request shape drift (wrong model ID string). Use exactly: gemini-2.5-pro, imagen-4.0-generate-001, veo-3.0-generate-001 / veo-3.0-fast-generate-001. 
Google Cloud
+3
Google Cloud
+3
Google Cloud
+3

Oversized bodies (huge image data URLs) or CORS mistakes. Keep payloads small; put big assets in Cloud Storage and pass signed URLs instead.

That’s the complete wiring diagram. If you want, I’ll plug these routes straight into MyAiMediaMgr (with your credit caps: Veo clips fixed at 8 s, fast by default, quality for Pro/Enterprise) and add the guardrails for personGeneration by region so you’re bulletproof when you scale